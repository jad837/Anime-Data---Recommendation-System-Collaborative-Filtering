{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"cde0f61c-f6ab-40b6-a757-c9394566aa62"},"source":"Here I want to make a simple recommender system to gauge the similarity between shows, users and to help me predict whether a user will enjoy a particular anime."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27f2680b-e691-70e4-e762-1bca22449d14"},"outputs":[],"source":"# Import relevant libraries \n\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport operator\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5c1d1b7-983a-daa8-27ab-d25011d1f669"},"outputs":[],"source":"anime = pd.read_csv('../input/anime.csv')\nrating = pd.read_csv('../input/rating.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"1f7f4306-2875-ee63-71e8-febb486605e6"},"source":"Before alteration the ratings dataset uses a \"-1\" to represent missing ratings.\nI'm replacing these placeholders with a null value because I will later be calculating \nthe average rating per user and don't want the average to be distorted"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30c96da8-d202-85cc-c546-47d31b39c04b"},"outputs":[],"source":"rating.rating.replace({-1: np.nan}, regex=True, inplace = True)\nrating.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42760207-0e77-e6cc-03c1-5cb3d2107991"},"outputs":[],"source":"# For this analysis I'm only interest in finding recommendations for the TV category\n\nanime_tv = anime[anime['type']=='TV']\nanime_tv.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"333be05a-5492-67a2-a64c-154a52ac060f"},"outputs":[],"source":"# Join the two dataframes on the anime_id columns\n\nmerged = rating.merge(anime_tv, left_on = 'anime_id', right_on = 'anime_id', suffixes= ['_user', ''])\nmerged.rename(columns = {'rating_user':'user_rating'}, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15cfaaf3-96c1-b14e-1742-11283997fc7a"},"outputs":[],"source":"# For computing reasons I'm limiting the dataframe length to 10,000 users\n\nmerged=merged[['user_id', 'name', 'user_rating']]\nmerged_sub= merged[merged.user_id <= 10000]\nmerged_sub.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"422fb6ee-644a-562f-ae27-99e759feaa6e"},"source":"For collaborative filtering we'll need to create a pivot table of users on one axis and tv show names along the other. The pivot table will help us in defining the similarity between users and shows to better predict who will like what."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7e42fe9-5cf2-c1fb-52ae-1d512c248a72"},"outputs":[],"source":"piv = merged_sub.pivot_table(index=['user_id'], columns=['name'], values='user_rating')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"978bc3ed-21c1-d563-13a8-f0b3056e8e7a"},"outputs":[],"source":"print(piv.shape)\npiv.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7632dde-c89f-3103-cb1e-36b23d530628"},"outputs":[],"source":"# Note: As we are subtracting the mean from each rating to standardize\n# all users with only one rating or who had rated everything the same will be dropped\n\n# Normalize the values\npiv_norm = piv.apply(lambda x: (x-np.mean(x))/(np.max(x)-np.min(x)), axis=1)\n\n\n# Drop all columns containing only zeros representing users who did not rate\npiv_norm.fillna(0, inplace=True)\npiv_norm = piv_norm.T\npiv_norm = piv_norm.loc[:, (piv_norm != 0).any(axis=0)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1784d9ce-6851-6388-dd05-0ff26493d268"},"outputs":[],"source":"# Our data needs to be in a sparse matrix format to be read by the following functions\n\npiv_sparse = sp.sparse.csr_matrix(piv_norm.values)"},{"cell_type":"markdown","metadata":{"_cell_guid":"56f295e3-153f-5628-f309-559b5494e30c"},"source":"These matrices show us the computed cosine similarity values \nbetween each user/user array pair and item/item array pair."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fa31240-3634-73e3-f0a1-9e8ecfb399dc"},"outputs":[],"source":"item_similarity = cosine_similarity(piv_sparse)\nuser_similarity = cosine_similarity(piv_sparse.T)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49228028-329f-cafa-6a31-87e27c2a7e54"},"outputs":[],"source":"# Inserting the similarity matricies into dataframe objects\n\nitem_sim_df = pd.DataFrame(item_similarity, index = piv_norm.index, columns = piv_norm.index)\nuser_sim_df = pd.DataFrame(user_similarity, index = piv_norm.columns, columns = piv_norm.columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5cd7919d-163a-7fd8-da8e-c06ce1d705f1"},"outputs":[],"source":"# This function will return the top 10 shows with the highest cosine similarity value\n\ndef top_animes(anime_name):\n    count = 1\n    print('Similar shows to {} include:\\n'.format(anime_name))\n    for item in item_sim_df.sort_values(by = anime_name, ascending = False).index[1:11]:\n        print('No. {}: {}'.format(count, item))\n        count +=1  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"725964d6-986f-d34b-a616-c10b4f504fc7"},"outputs":[],"source":"# This function will return the top 5 users with the highest similarity value \n\ndef top_users(user):\n    \n    if user not in piv_norm.columns:\n        return('No data available on user {}'.format(user))\n    \n    print('Most Similar Users:\\n')\n    sim_values = user_sim_df.sort_values(by=user, ascending=False).loc[:,user].tolist()[1:11]\n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:11]\n    zipped = zip(sim_users, sim_values,)\n    for user, sim in zipped:\n        print('User #{0}, Similarity value: {1:.2f}'.format(user, sim)) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"876a5010-0e99-7aa8-5e9c-e704181b81a3"},"outputs":[],"source":"# This function constructs a list of lists containing the highest rated shows per similar user\n# and returns the name of the show along with the frequency it appears in the list\n\ndef similar_user_recs(user):\n    \n    if user not in piv_norm.columns:\n        return('No data available on user {}'.format(user))\n    \n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:11]\n    best = []\n    most_common = {}\n    \n    for i in sim_users:\n        max_score = piv_norm.loc[:, i].max()\n        best.append(piv_norm[piv_norm.loc[:, i]==max_score].index.tolist())\n    for i in range(len(best)):\n        for j in best[i]:\n            if j in most_common:\n                most_common[j] += 1\n            else:\n                most_common[j] = 1\n    sorted_list = sorted(most_common.items(), key=operator.itemgetter(1), reverse=True)\n    return sorted_list[:5]    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55c0e789-caa4-3c6d-c062-f58fb8268a2e"},"outputs":[],"source":"# This function calculates the weighted average of similar users\n# to determine a potential rating for an input user and show\n\ndef predicted_rating(anime_name, user):\n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:1000]\n    user_values = user_sim_df.sort_values(by=user, ascending=False).loc[:,user].tolist()[1:1000]\n    rating_list = []\n    weight_list = []\n    for j, i in enumerate(sim_users):\n        rating = piv.loc[i, anime_name]\n        similarity = user_values[j]\n        if np.isnan(rating):\n            continue\n        elif not np.isnan(rating):\n            rating_list.append(rating*similarity)\n            weight_list.append(similarity)\n    return sum(rating_list)/sum(weight_list)    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50988cb1-4034-4316-2638-d0b5954b0c26"},"outputs":[],"source":"top_animes('Cowboy Bebop')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"360f71b5-86dd-166a-a235-4ae662449a77"},"outputs":[],"source":"top_users(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99affa09-1a22-e7d0-7f39-f785cd8b89a4"},"outputs":[],"source":"similar_user_recs(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"007764c2-bafc-a061-d9a8-eb03a12e629f"},"outputs":[],"source":"predicted_rating('Cowboy Bebop', 3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6c0f46ae-dcab-82d5-0023-fecf156b0a5f"},"source":"Below we'll see how the predict_rating function performs compared to the observed rated values for user 3."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1be578f6-4990-2a5a-a121-c4f2e63279a3"},"outputs":[],"source":"# Creates a list of every show watched by user 3\n\nwatched = piv.T[piv.loc[3,:]>0].index.tolist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"716aba34-a60a-2418-822c-8143d512da74"},"outputs":[],"source":"# Make a list of the squared errors between actual and predicted value\n\nerrors = []\nfor i in watched:\n    actual=piv.loc[3, i]\n    predicted = predicted_rating(i, 3)\n    errors.append((actual-predicted)**2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c05ec3d-7874-6ff2-c0a9-7d732b3485cf"},"outputs":[],"source":"# This is the average squared error for user 3\nnp.mean(errors)"},{"cell_type":"markdown","metadata":{"_cell_guid":"00398617-041c-d135-1ca7-5566e49d4e8e"},"source":"This is my first Kaggle submission and python project in general so any helpful guidance about best practices or efficiency would be much appreciated. Thank you!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}